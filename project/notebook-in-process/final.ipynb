{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of mouse behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pyautogui as pag\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import json as js\n",
    "\n",
    "\n",
    "def readreward(self):\n",
    "    search_dir = \"/Users/guowei/Downloads/js\"\n",
    "    os.chdir(search_dir)\n",
    "    files = filter(os.path.isfile, os.listdir(search_dir))\n",
    "    files = [os.path.join(search_dir, f) for f in files]  # add path to each file\n",
    "    files.sort(key=lambda x: os.path.getmtime(x))\n",
    "\n",
    "    new_file = files[-1]\n",
    "    #         print(lists)\n",
    "    path = os.path.join(search_dir, new_file)\n",
    "    #         print(path)\n",
    "    f = open(path, 'r')\n",
    "    load_dict = js.loads(f.read().encode('utf-8').strip())\n",
    "    reward = load_dict['reward']\n",
    "    return reward\n",
    "\n",
    "\n",
    "def readreward2(self):\n",
    "    search_dir = \"/Users/guowei/Downloads/js\"\n",
    "    os.chdir(search_dir)\n",
    "    files = filter(os.path.isfile, os.listdir(search_dir))\n",
    "    files = [os.path.join(search_dir, f) for f in files]  # add path to each file\n",
    "    files.sort(key=lambda x: os.path.getmtime(x))\n",
    "    new_file = files[-1]\n",
    "    path = os.path.join(search_dir, new_file)\n",
    "    f = open(path, 'r')\n",
    "    load_dict = js.loads(f.read().encode('utf-8').strip())\n",
    "    final_score = load_dict['updated_score']\n",
    "\n",
    "    old_file = files[-3]  # second file\n",
    "    path1 = os.path.join(search_dir, old_file)\n",
    "    f1 = open(path1, 'r')\n",
    "    second_score = js.loads(f1.read())['updated_score']\n",
    "    reward = final_score - second_score\n",
    "    return reward\n",
    "\n",
    "\n",
    "def deletefile(self):\n",
    "    test_report = \"/Users/guowei/Downloads/js\"\n",
    "    lists = os.listdir(test_report)\n",
    "    #         filelist = [ f for f in lists ]\n",
    "    for f in lists:\n",
    "        #             print(f)\n",
    "        os.remove(os.path.join(test_report, f))\n",
    "\n",
    "\n",
    "\n",
    "def get_random_action():\n",
    "    a = random.randint(0,7)\n",
    "    return a\n",
    "\n",
    "def next_line(a):\n",
    "    pag.moveTo(150,370- a*10,0.1)\n",
    "    print(\"back to start\"+str(pag.position()))\n",
    "\n",
    "def one_movement(x,a):\n",
    "    pag.moveTo(x+10,370- a*10,0.1)\n",
    "    print(pag.position())\n",
    "\n",
    "def get_next_state(s,a):\n",
    "    a = get_random_action()\n",
    "\n",
    "def draw_step(s,a,x_coors,y_coors):\n",
    "\n",
    "    step = int ((s-1)/488)\n",
    "\n",
    "    sum = s - step *488\n",
    "\n",
    "    x = int((sum-1)/8)\n",
    "\n",
    "    y = sum - x*8\n",
    "\n",
    "    x_coor = 150 + 10*x\n",
    "\n",
    "    y_coor = 370 - 10* (y-1)\n",
    "\n",
    "    x_coors += [x_coor]\n",
    "    y_coors += [y_coor]\n",
    "\n",
    "    if step == 2 and x == 60 :\n",
    "        pag.press('esc')\n",
    "\n",
    "        s = 488*3 +1\n",
    "        R = 0\n",
    "        status = True\n",
    "        return s , R , status, x_coors[182],y_coors[182], x_coors[181],y_coors[181], x_coors[121],y_coors[121], x_coors[121],y_coors[121],\n",
    "\n",
    "\n",
    "    elif step == 1 and x == 60 :\n",
    "        pag.press('esc')\n",
    "        pag.press('p')\n",
    "        pag.typewrite('p')\n",
    "        time.sleep(2)\n",
    "\n",
    "        R = 0\n",
    "        status = False\n",
    "\n",
    "        s = 488*2 + a+1\n",
    "\n",
    "        next_line(a)\n",
    "\n",
    "        pag.press('enter')\n",
    "\n",
    "        return s, R, status, x_coors[121],y_coors[121],x_coors[120],y_coors[120],x_coors[60],y_coors[60],x_coors[60],y_coors[60]\n",
    "\n",
    "    elif step ==0 and x ==60 :\n",
    "        pag.press('esc')\n",
    "        pag.press('p')\n",
    "        pag.typewrite('p')\n",
    "        time.sleep(2)\n",
    "\n",
    "        R = 0\n",
    "        status = False\n",
    "        s =488+a\n",
    "\n",
    "        next_line(a)\n",
    "        pag.press('enter')\n",
    "        return s, R, status, x_coors[60], y_coors[60], x_coors[59], y_coors[59], 750, 370, 750,370\n",
    "\n",
    "    elif step ==2 and x ==59 :\n",
    "\n",
    "        R =  3\n",
    "        status = False\n",
    "        s = 488 +488 +480 + a+1\n",
    "\n",
    "        one_movement(x_coor,a)\n",
    "        return s,R,status, x_coors[181],y_coors[181],x_coors[180],y_coors[180],x_coors[120],y_coors[120], x_coors[121],y_coors[121]\n",
    "\n",
    "    elif step == 1 and x ==59 :\n",
    "        R =2\n",
    "        status = False\n",
    "        s = 488  + 480 + a+1\n",
    "        one_movement(x_coor, a)\n",
    "\n",
    "        return s,R,status,x_coors[120],y_coors[120],x_coors[119],y_coors[119],x_coors[59],y_coors[59],x_coors[60],y_coors[60]\n",
    "\n",
    "    elif step == 0 and x ==59:\n",
    "\n",
    "        R = 1\n",
    "        status = False\n",
    "\n",
    "        s = 480 + a+1\n",
    "\n",
    "        one_movement(x_coor, a)\n",
    "        return s, R, status, x_coors[59], y_coors[59], x_coors[58], y_coors[58], 740, 370, 750, 370\n",
    "\n",
    "    elif step ==0 and x == 0 :\n",
    "\n",
    "\n",
    "        R = 0\n",
    "        status = False\n",
    "        s = (x+1)* 8 + a + 1\n",
    "        pag.press('enter')\n",
    "\n",
    "\n",
    "\n",
    "        one_movement(x_coor, a)\n",
    "\n",
    "        return s, R ,status, x_coors[0],y_coors[0], x_coors[0],y_coors[0], 150,370, 160,370\n",
    "\n",
    "    elif x== 0 :\n",
    "\n",
    "        R = 0\n",
    "        status = False\n",
    "        s = step*488 + (x+1) * 8 + a + 1\n",
    "        one_movement(x_coor, a)\n",
    "\n",
    "        return s, R, status, x_coors[x + 61 * step], y_coors[x + 61 * step], x_coors[x + 61 * step], y_coors[\n",
    "            x + 61 * step], x_coors[x + 61 * (step - 1)], y_coors[x + 61 * (step - 1)], x_coors[\n",
    "                   x + 61 * (step - 1) + 1], y_coors[x + 61 * (step - 1) + 1]\n",
    "\n",
    "\n",
    "\n",
    "    elif step == 0 :\n",
    "\n",
    "        R = 0\n",
    "        status = False\n",
    "        s= (x+1)* 8 + a+1\n",
    "\n",
    "        one_movement(x_coor,a)\n",
    "        return s,R, status, x_coors[x],y_coors[x], x_coors[x-1], y_coors[x-1], x_coors[x],370, x_coors[x]+10,370\n",
    "\n",
    "    else :\n",
    "        R = 0\n",
    "\n",
    "        status = False\n",
    "        s = step * 488 + (x+1 )* 8 + a + 1\n",
    "        one_movement(x_coor, a)\n",
    "\n",
    "        return s, R, status, x_coors[x + 61 * step], y_coors[x + 61 * step], x_coors[x + 61 * step-1], y_coors[\n",
    "            x + 61 * step-1], x_coors[x + 61 * (step - 1)], y_coors[x + 61 * (step - 1)], x_coors[\n",
    "                   x + 61 * (step - 1) + 1], y_coors[x + 61 * (step - 1) + 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def masking_game(iteration):\n",
    "    time.sleep(3)\n",
    "    for i in range(20):\n",
    "        pag.typewrite('p')\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(iteration):\n",
    "\n",
    "        terminal = False\n",
    "\n",
    "        x_coors = []\n",
    "        y_coors = []\n",
    "        states = []\n",
    "\n",
    "\n",
    "        a = get_random_action()\n",
    "\n",
    "        s = a\n",
    "        pag.moveTo(150, 370-a*10)\n",
    "\n",
    "        #x_coors +=[150]\n",
    "        #y_coors +=[370 - 10* a ]\n",
    "\n",
    "        states += [s]\n",
    "\n",
    "        while terminal == False:\n",
    "\n",
    "            a = get_random_action()\n",
    "\n",
    "            s,R,terminal,x1,y1,x2,y2,x3,y3,x4,y4 = draw_step(s,a,x_coors,y_coors)\n",
    "            #1: 当前点 2: 左边 3:下面 4:右下\n",
    "            print(s,R, x1,y1,x2,y2,x3,y3,x4,y4)\n",
    "\n",
    "\n",
    "\n",
    "        pag.press('p')\n",
    "        pag.typewrite('p')\n",
    "        pag.press('n')\n",
    "        pag.typewrite('n')\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#masking_game(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN-code-zhihu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9 # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01 # final value of epsilon\n",
    "REPLAY_SIZE = 10000 # experience replay buffer size\n",
    "BATCH_SIZE = 32 # size of minibatch\n",
    "\n",
    "class DQN():\n",
    "  # DQN Agent\n",
    "    def __init__(self):\n",
    "    # init experience replay\n",
    "        self.replay_buffer = deque()\n",
    "    # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_dim = 4\n",
    "#         self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = 8\n",
    "\n",
    "        self.create_Q_network()\n",
    "        self.create_training_method()\n",
    "\n",
    "    # Init session\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def create_Q_network(self):\n",
    "\n",
    "    #initial network weights\n",
    "        W1 = self.weight_variable([self.state_dim, 8])\n",
    "        b1 = self.bias_variable([8])\n",
    "        W2 = self.weight_variable([8,12])\n",
    "        b2 = self.bias_variable([12])\n",
    "        W3 = self.weight_variable([12,self.action_dim])\n",
    "        b3 = self.bias_variable([self.action_dim])\n",
    "\n",
    "    #input layer\n",
    "        self.state_input = tf.placeholder('float',[None, self.state_dim])\n",
    "\n",
    "    #hidden layers\n",
    "        h1 = tf.nn.relu(tf.matmul(self.state_input, W1) + b1)\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "\n",
    "    #Q Value layer\n",
    "        self.Q_value = tf.matmul(h2, W3) + b3\n",
    "    \n",
    "    \n",
    "\n",
    "#   def create_Q_network(self):\n",
    "#     # network weights\n",
    "#     W1 = self.weight_variable([self.state_dim,20])\n",
    "#     b1 = self.bias_variable([20])\n",
    "#     W2 = self.weight_variable([20,self.action_dim])\n",
    "#     b2 = self.bias_variable([self.action_dim])\n",
    "#     # input layer\n",
    "#     self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n",
    "#     # hidden layers\n",
    "#     h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n",
    "#     # Q Value layer\n",
    "#     self.Q_value = tf.matmul(h_layer,W2) + b2\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot presentation\n",
    "        self.y_input = tf.placeholder(\"float\",[None])\n",
    "#         Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\n",
    "        Q_action = tf.reduce_sum(self.Q_value[int(self.action_input - 1)])\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "\n",
    "    def perceive(self,state,action,reward,next_state,done):\n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action-1] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "    # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "\n",
    "    # Step 2: calculate y\n",
    "        y_batch = []\n",
    "        Q_value_batch = self.Q_value.eval(feed_dict={self.state_input:next_state_batch})\n",
    "        for i in range(0,BATCH_SIZE):\n",
    "            done = minibatch[i][4]\n",
    "            if done:\n",
    "                y_batch.append(reward_batch[i])\n",
    "            else :\n",
    "                y_batch.append(reward_batch[i] + GAMMA * np.max(Q_value_batch[i]))\n",
    "\n",
    "        self.optimizer.run(feed_dict={\n",
    "              self.y_input:y_batch,\n",
    "              self.action_input:action_batch,\n",
    "              self.state_input:state_batch})\n",
    "\n",
    "    def egreedy_action(self,state):\n",
    "        Q_value = self.Q_value.eval(feed_dict = {self.state_input:[state]})[0]\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randint(0,self.action_dim - 1)\n",
    "        else:\n",
    "            return np.argmax(Q_value)\n",
    "\n",
    "        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/10000\n",
    "\n",
    "    def action(self,state):\n",
    "        return np.argmax(self.Q_value.eval(feed_dict = {self.state_input:[state]})[0])\n",
    "\n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(self,shape):\n",
    "        initial = tf.constant(0.01, shape = shape)\n",
    "        return tf.Variable(initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "# ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 10 # Episode limitation\n",
    "STEP = 183 # Step limitation in an episode\n",
    "TEST = 10 # The number of experiment test every 100 episode\n",
    "\n",
    "def main():\n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "#     env = gym.make(ENV_NAME)\n",
    "    agent = DQN()\n",
    "\n",
    "    for episode in xrange(EPISODE):\n",
    "        s = get_random_action()\n",
    "        for step in range(STEP):\n",
    "#         for step in xrange(STEP):\n",
    "            action = agent.egreedy_action(state) # e-greedy action for train\n",
    "            next_state,reward,done,_,_,_,_,_,_,_,_, = draw_step(s = s, a = action, x_coors = [], y_coors = [])\n",
    "      # Define reward for agent\n",
    "            #reward_agent = -1 if done else 0.1\n",
    "            agent.perceive(state,action,reward,next_state,done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "#     # Test every 100 episodes\n",
    "#         if episode % 100 == 0:\n",
    "#             total_reward = 0\n",
    "#             for i in xrange(TEST):\n",
    "#                 state = env.reset()\n",
    "#                 for j in xrange(STEP):\n",
    "#                     env.render()\n",
    "#                     action = agent.action(state) # direct action for test\n",
    "#                     state,reward,done,_ = env.step(action)\n",
    "#                     total_reward += reward\n",
    "#                     if done:\n",
    "#                         break\n",
    "#             ave_reward = total_reward/TEST\n",
    "#             print('episode: ',episode,'Evaluation Average Reward:',ave_reward)\n",
    "#             if ave_reward >= 200:\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'mul'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4d08123813f6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# initialize OpenAI Gym env and dqn agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     env = gym.make(ENV_NAME)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c4ba0501e9ec>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_Q_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Init session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c4ba0501e9ec>\u001b[0m in \u001b[0;36mcreate_training_method\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# one hot presentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mQ_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_input\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'mul'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
